{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evidence for model**\n",
    "$$\\log P(\\{\\log L_i \\} \\mid \\bm{\\theta}) \\propto \\log P(\\bm{\\theta} \\mid \\{\\log L_i \\}) \\approx\n",
    "\\log P'(\\bm{\\theta}) = \\log P'(\\bm{\\theta}_0) - \\frac{1}{2} (\\bm{\\theta} - \\bm{\\theta}_0)^\\intercal \\bm{A} (\\bm{\\theta} - \\bm{\\theta}_0)$$\n",
    "$$ \\log Z = \\log L_\\mathrm{max} + \\frac{1}{2} \\log |2\\pi\\Sigma| - \\log V_\\pi $$\n",
    "$$ \\log Z = \\log P'(\\bm{\\theta}_0)  + \\frac{1}{2} \\log |2\\pi H^{-1}| - \\log V_\\pi $$\n",
    "$$ Z = P'(\\bm{\\theta}_0) \\sqrt{\\frac{2\\pi |H^{-1}|}{V_\\pi}} $$\n",
    "$$ \\log Z \\sim \\log P'(\\bm{\\theta}_0) - \\frac{1}{2}\\log|H| $$\n",
    "\n",
    "Remember $ \\log P(\\bm{\\theta} \\mid \\{\\log L_i \\}$ and hence $\\log P'(\\bm{\\theta}_0)$ has a normalising factor $-\\frac{1}{2} \\log |\\Sigma| = +\\frac{1}{2} \\log |\\Sigma^{-1}|$ in front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from scipy.optimize import minimize, least_squares\n",
    "\n",
    "# Load test samples\n",
    "from aeons.tools import pickle_in\n",
    "samples_g_1, samples_c_1, samples_w_1 = pickle_in(\"../test_samples/samples_1.pickle\")\n",
    "\n",
    "from aeons.covariance import logX_mu, logX_Sigmainv, points_at_iteration, X_mu, X_Sigma, X_Sigmainv, data_at_iteration\n",
    "from aeons.true_distribution import generate_Xs\n",
    "from aeons.lm_partial import analytic_lm_params\n",
    "from aeons.bayes import logPr_bayes, logPr_laplace, logPr_gaussian, minimise_ls, minimise_bayes, minimise_gaussian\n",
    "from aeons.likelihoods import likelihood, linear_like, quad_like, log_like, simple_like, middle_like, full_like\n",
    "linear, quad, log = linear_like(), quad_like(), log_like()\n",
    "simple, middle, full = simple_like(), middle_like(), full_like()\n",
    "simple_log, middle_log, full_log = simple_like(logX=True), middle_like(logX=True), full_like(logX=True)\n",
    "from aeons.hessian import hess_autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_log_like():\n",
    "    def func(X, theta):\n",
    "        logLmax, d, sigma = theta\n",
    "        X = np.exp(X)\n",
    "        return logLmax - X**(2/d)/(2*sigma**2)\n",
    "    def inverse(logL, theta):\n",
    "        logLmax, d, sigma = theta\n",
    "        return np.log((2*sigma**2 * (logLmax - logL))**(d/2))\n",
    "    def prime(X, theta):\n",
    "        logLmax, d, sigma = theta\n",
    "        X = np.exp(X)\n",
    "        return - 1/(d*sigma**2) * X**(2/d)\n",
    "    return likelihood(func, inverse, prime)\n",
    "full_log = full_log_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def full_log_like_torch():\n",
    "    def func(X, theta):\n",
    "        logLmax, d, sigma = theta\n",
    "        X = torch.exp(X)\n",
    "        return logLmax - X**(2/d)/(2*sigma**2)\n",
    "    def inverse(logL, theta):\n",
    "        logLmax, d, sigma = theta\n",
    "        logL = torch.tensor(logL)\n",
    "        return torch.log((2*sigma**2 * (logLmax - logL))**(d/2))\n",
    "    def prime(X, theta):\n",
    "        logLmax, d, sigma = theta\n",
    "        X = torch.exp(torch.tensor(X))\n",
    "        return - 1/(d*sigma**2) * X**(2/d)\n",
    "    return likelihood(func, inverse, prime)\n",
    "full_log_torch = full_log_like_torch()\n",
    "\n",
    "def logPr_bayes_torch(logL, likelihood, mean, covinv, theta):\n",
    "    \"\"\"likelihood = f(X_i, theta)\"\"\"\n",
    "    Xstar = likelihood.inverse(logL, theta)\n",
    "    log_abs_fprimes = torch.log(abs(likelihood.prime(Xstar, theta)))\n",
    "    return - torch.sum(log_abs_fprimes)- 1/2 * (Xstar - mean).T @ covinv @ (Xstar - mean)\n",
    "\n",
    "def minimise_bayes(logL, likelihood, mean, covinv, x0):\n",
    "    def func(theta):\n",
    "        return - logPr_bayes(logL, likelihood, mean, covinv, theta)\n",
    "    solution = minimize(func, x0, method='Nelder-Mead')\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logZ(y, likelihood, mean, covinv, theta_max, H):\n",
    "    eigvals_covinv = np.linalg.eigvals(covinv)\n",
    "    logPr_max = logPr_bayes(y, likelihood, mean, covinv, theta_max) + 1/2 * np.log(eigvals_covinv).sum()\n",
    "    return logPr_max - 1/2 * np.log(abs(np.linalg.det(H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk = 500 * np.ones(1000)\n",
    "mean_X = X_mu(nk)\n",
    "covinv_X = X_Sigmainv(nk)\n",
    "mean_logX = logX_mu(nk)\n",
    "covinv_logX = logX_Sigmainv(nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zixiao/Documents/III/project/aeons/aeons/likelihoods.py:111: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return - 1/(d*sigma**2) * X**(2/d - 1)\n"
     ]
    }
   ],
   "source": [
    "# # nk, L = data_at_iteration(samples_w_1, 1000)\n",
    "# mean = X_mu(nk)\n",
    "# covinv = X_Sigmainv(nk)\n",
    "# theta = minimise_bayes(L, full, mean, covinv, [1, 10, 0.1]).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [ 0.98336328 10.78177846  0.10000351]\n",
      "logX: [ 0.12138    10.69683441  0.1008769 ]\n",
      "bayes factor: 1.7396874238412947\n"
     ]
    }
   ],
   "source": [
    "Xs = generate_Xs(nk)\n",
    "theta_true = np.array([1, 10, 0.1])\n",
    "y = full.func(Xs, theta_true)\n",
    "theta_logX = minimise_bayes(y, full_log, mean_logX, covinv_logX, theta_true).x\n",
    "theta_X = minimise_bayes(y, full, mean_X, covinv_X, theta_true).x\n",
    "X_X = full.inverse(y, theta_X)\n",
    "X_logX = full.inverse(y, theta_logX)\n",
    "H_X = hess_autograd(y, full, mean_X, covinv_X, theta_X)\n",
    "H_logX = hess_autograd(y, full_log_torch, mean_logX, covinv_logX, theta_logX)\n",
    "ratio = np.exp(logZ(y, full, mean_X, covinv_X, theta_X, H_X) - logZ(y, full_log, mean_logX, covinv_logX, theta_logX, H_logX))\n",
    "print(f'X: {theta_X}')\n",
    "print(f'logX: {theta_logX}')\n",
    "print(f'bayes factor: {ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1592191/333922539.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  ratio = np.exp(logZ(y, full, mean_X, covinv_X, theta_X, H_X) - logZ(y, full, mean_X, covinv_X, [2, 12, 0.8], H_X))\n"
     ]
    }
   ],
   "source": [
    "ratio = np.exp(logZ(y, full, mean_X, covinv_X, theta_X, H_X) - logZ(y, full, mean_X, covinv_X, [2, 12, 0.8], H_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6548ad9276b24d19be01f26df8cf45e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='frame', options=('full', 1, 2, 3), value='full'), Output()), _â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "@interact(frame=widgets.RadioButtons(options=['full', 1, 2, 3]))\n",
    "def plot(frame):\n",
    "    plt.plot(Xs, y, 'x', ms=1, color='deepskyblue')\n",
    "    plt.plot(X_X, y, color='black', label='X')\n",
    "    plt.plot(X_logX, y, color='gray', label='logX')\n",
    "    if frame == 'full':\n",
    "        pass\n",
    "    elif frame == 1:\n",
    "        plt.xlim(0.2, 0.4)\n",
    "        plt.ylim(-40, -35)\n",
    "    elif frame == 2:\n",
    "        plt.xlim(0.4, 0.46)\n",
    "        plt.ylim(-42, -40)\n",
    "    else:\n",
    "        plt.xlim(0.6, 0.8)\n",
    "        plt.ylim(-47.5, -43)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f04b3df41c937fb946d36e0c914ccce305e67d8dbbe371762424715e8a219369"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
